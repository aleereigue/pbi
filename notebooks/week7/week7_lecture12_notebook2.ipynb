{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b7c805-32ba-429c-a3fc-c415a05033c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Programming for Biomedical Informatics\n",
    "#### Week 7 - Network Construction Techniques\n",
    "\n",
    "Constructing networks that are useful representations of the underlying biological data is a complex task. In this notebook we will explore some key concepts that are used to incoporate data into networks and then refine those using a selection of methodologies.\n",
    "Quantifying the impact of the assumptions and decisions made in the network construction and refinement process is a key part of the experimental analysis of networks. This is often confounded by the lack of ground-truth data upon which to make decisions.\n",
    "\n",
    "Thanks to Sebestyen Kamp who developed parts of these scripts for a workshop on networks presented at ISMB2024 in Montreal, Canada. We have collected a lot of functions we use in this code into an accompaying graph_functions.py file to make this script easier to read and make function re-use simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1907862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Biomedical Networks'''\n",
    "# standard libraries\n",
    "import os\n",
    "import pickle\n",
    "import graph_functions as gf\n",
    "\n",
    "# scientific and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "# mygene is a library for querying gene information (though you could use eUtils etc.)\n",
    "import mygene\n",
    "\n",
    "# graph and network libraries\n",
    "import networkx as nx\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# some deprecation warnings that are safe to ignore can be silenced using the warnings library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96698571",
   "metadata": {},
   "source": [
    "We're going to be looking at some gene expression data from the cancer genome atlas.\n",
    "\n",
    "Note that it has multi-modal data - we will look at this in later lectures. We're going to concentrate on gene expression in this notebook\n",
    "\n",
    "- **Title**: The Cancer Genome Atlas Lung Adenocarcinoma (TCGA-LUAD)\n",
    "- **Main Focus**: Study of lung adenocarcinoma (a common type of lung cancer)\n",
    "- **Data Collected**: Genomic, epigenomic, transcriptomic, and proteomic data from lung adenocarcinoma samples\n",
    "- **Disease Types**:\n",
    "  - Acinar Cell Neoplasms\n",
    "  - Adenomas and Adenocarcinomas\n",
    "  - Cystic, Mucinous, and Serous Neoplasms\n",
    "- **Number of Cases**: 585 (498 with transcriptomic data)\n",
    "- **Data Accessibility**: Available on the NIH-GDC Data Portal\n",
    "\n",
    "- **Link**: [TCGA-LUAD Project Page](https://portal.gdc.cancer.gov/projects/TCGA-LUAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce5339-5c18-48c0-b95b-d21196029943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gene expression data from a pickle file\n",
    "'''a pickle file is a serialized python object that can be saved to disk and loaded back into memory\n",
    "these can be very useful for sharing python objects. In this case we have a dictionary with the gene \n",
    "expression data'''\n",
    "\n",
    "with open(\"ISMB_TCGA_GE.pkl\", 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# print the keys of the dictionary\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ac5a0",
   "metadata": {},
   "source": [
    "In order to construct a biological network, we are going to first:\n",
    "- examine the TCGA metadata \n",
    "- come up with useful strategies to tackle the large data size \n",
    "- create the basis of a biological network\n",
    "\n",
    "We're going to first familiarise ourselves with the data by looking at the meta-data that \\\n",
    "comes with the gene expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first few rows of the gene expression meta-data\n",
    "metadata = data[\"datMeta\"]\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique patient identifiers in the 'patient' column of the dataFrame\n",
    "print(f'There are',data[\"datMeta\"][\"patient\"].unique().size,'patient samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc08cd8-49b9-47d7-b46e-a47c67b074c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'sample_type' column of the 'datMeta' DataFrame\n",
    "print(data[\"datMeta\"]['sample_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba3ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's usually a good idea to us a dimensionality reduction method to look for outliers and distributions of samples\n",
    "# Here we will do a basic PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "expression_data = data['datExpr']\n",
    "\n",
    "# scale features, run PCA on 2-dimensions\n",
    "X = expression_data.values\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "pcs = pca.fit_transform(X_scaled)\n",
    "sample_pca = pd.DataFrame(pcs, columns=['PC1', 'PC2'], index=expression_data.index)\n",
    "\n",
    "# add the sample_type so we can see if cancerous and non-cancerous samples are split\n",
    "sample_pca = sample_pca.join(metadata['sample_type'])\n",
    "sample_pca.index.name = 'sample_id'\n",
    "print(sample_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac37312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pca results\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# map statuses to distinct colors\n",
    "statuses = list(sample_pca['sample_type'].unique())\n",
    "cmap = plt.get_cmap('Set1')\n",
    "color_map = {s: cmap(i % cmap.N) for i, s in enumerate(statuses)}\n",
    "colors = sample_pca['sample_type'].map(color_map)\n",
    "\n",
    "# create figure and axes, scatter and attach colorbar to that axes\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "sc = ax.scatter(sample_pca['PC1'], sample_pca['PC2'], s=50, edgecolor='k',c=list(colors))\n",
    "\n",
    "# simple legend with colored patches\n",
    "legend_elements = [Patch(facecolor=col, edgecolor='k', label=label) \n",
    "                   for label, col in color_map.items()]\n",
    "ax.legend(handles=legend_elements, title='sample_type')\n",
    "\n",
    "# labels and colorbar\n",
    "ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "ax.set_title('PCA')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6433b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to focus on genes that are dysregulated between the two sample status classes we will perform differential expression analysis on them\n",
    "# the data from TCGA are already normalised which means they are no loner integers.\n",
    "# although it's not best practice we will simply round to the nearest integer to allow us to import these data into DESeq2\n",
    "# in a full analysis we would go back to the raw count data per sample and re-build the data matrix\n",
    "expression_data = expression_data.round().astype(int)\n",
    "\n",
    "# look at the expression_data\n",
    "expression_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c76726",
   "metadata": {},
   "source": [
    "We are going to visualise various metadata attributes such as race, gender, sample type, cigarettes per day, and smoking status by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d80d8-48ff-4c8f-894e-f3ad6db4f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the distributions of various data labels\n",
    "\n",
    "# Set up the figure and axes for a 2-column layout\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 18))\n",
    "fig.suptitle('Metadata Distributions', fontsize=20, y=0)\n",
    "\n",
    "# Plot 1: Distribution of Race\n",
    "sns.countplot(ax=axes[0, 0], x='race', data=data['datMeta'], palette='viridis')\n",
    "axes[0, 0].set_title('Distribution of Race')\n",
    "axes[0, 0].set_xlabel('Race')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Gender Distribution\n",
    "sns.countplot(ax=axes[0, 1], x='gender', data=data['datMeta'], palette='magma')\n",
    "axes[0, 1].set_title('Gender Distribution')\n",
    "axes[0, 1].set_xlabel('Gender')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Plot 3: Sample Type Distribution\n",
    "sns.countplot(ax=axes[1, 0], x='sample_type', data=data['datMeta'], palette='plasma')\n",
    "axes[1, 0].set_title('Sample Type Distribution')\n",
    "axes[1, 0].set_xlabel('Sample Type')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Distribution of Cigarettes Per Day\n",
    "sns.histplot(ax=axes[1, 1], data=data['datMeta']['cigarettes_per_day'], kde=True, color='blue')\n",
    "axes[1, 1].set_title('Distribution of Cigarettes Per Day')\n",
    "axes[1, 1].set_xlabel('Cigarettes Per Day')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 5: Smoking Status by Gender\n",
    "sns.countplot(ax=axes[2, 0], x='Smoked', hue='gender', data=data['datMeta'], palette='coolwarm')\n",
    "axes[2, 0].set_title('Smoking Status by Gender')\n",
    "axes[2, 0].set_xlabel('Smoking Status')\n",
    "axes[2, 0].set_ylabel('Count')\n",
    "axes[2, 0].legend(title='Gender')\n",
    "\n",
    "axes[2, 1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159dbc4",
   "metadata": {},
   "source": [
    "This dataset contains gene expression levels for various samples, identified by their TCGA (The Cancer Genome Atlas) codes.  \n",
    "Each row represents a different sample, while each column represents a different gene, identified by its Ensembl gene ID.  \n",
    "The values in the table are the expression levels of the genes for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set up a DESEq2 experiment to compare cancer vs. non-cancer taking into account smoking status\n",
    "# this is like the bi-factor analysis we did last week\n",
    "\n",
    "from pydeseq2.dds import DeseqDataSet\n",
    "from pydeseq2.default_inference import DefaultInference\n",
    "from pydeseq2.ds import DeseqStats\n",
    "from pydeseq2.utils import *\n",
    "\n",
    "inference = DefaultInference(n_cpus=8)\n",
    "dds = DeseqDataSet(\n",
    "    counts=expression_data,\n",
    "    metadata=metadata,\n",
    "    design=\"~sample_type+Smoked\",\n",
    "    refit_cooks=True,\n",
    "    inference=inference,\n",
    ")\n",
    "\n",
    "print(dds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we want to focus on the network construction elements we will run a one-shot DESeq2 analysis without exploring intermediate steps\n",
    "dds.deseq2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we setup the comparison to be made\n",
    "stat_res = DeseqStats(dds, contrast=['sample_type','Primary Tumor','Solid Tissue Normal'], inference=inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a71259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the result slot\n",
    "stat_res.summary()\n",
    "results = stat_res.results_df\n",
    "\n",
    "# sort the data by adjusted p-value\n",
    "sorted_results = results.sort_values(by='padj', ascending=True)\n",
    "sorted_results.reset_index(inplace=True)\n",
    "sorted_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to restrict the differentially expressed genes retained by setting cut-offs for padj and fold-change\n",
    "significant = sorted_results[(sorted_results['padj'] <= 0.05) & (abs(sorted_results['log2FoldChange']) >= 0.8)]\n",
    "print(f'There are',len(significant),'significantly differentially expressed genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ids of the genes to be retained\n",
    "sig_genes = list(significant['_row'])\n",
    "\n",
    "#select only the gene columns that are significant\n",
    "significant_genes = expression_data[expression_data.columns.intersection(sig_genes)]\n",
    "print(significant_genes.shape)\n",
    "\n",
    "# so for 498 patients we have a fingerprint of 357 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bd523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for interest let's conduct a PCA on these\n",
    "# scale features, run PCA on 2-dimensions\n",
    "X = significant_genes.values\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "pcs = pca.fit_transform(X_scaled)\n",
    "sample_pca = pd.DataFrame(pcs, columns=['PC1', 'PC2'], index=significant_genes.index)\n",
    "\n",
    "# add the sample_type so we can see if cancerous and non-cancerous samples are split\n",
    "sample_pca = sample_pca.join(metadata['sample_type'])\n",
    "\n",
    "#visualise\n",
    "sample_pca.index.name = 'sample_id'\n",
    "\n",
    "# map statuses to distinct colors\n",
    "statuses = list(sample_pca['sample_type'].unique())\n",
    "cmap = plt.get_cmap('Set1')\n",
    "color_map = {s: cmap(i % cmap.N) for i, s in enumerate(statuses)}\n",
    "colors = sample_pca['sample_type'].map(color_map)\n",
    "\n",
    "# create figure and axes, scatter and attach colorbar to that axes\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "sc = ax.scatter(sample_pca['PC1'], sample_pca['PC2'], s=50, edgecolor='k',c=list(colors))\n",
    "\n",
    "# simple legend with colored patches\n",
    "legend_elements = [Patch(facecolor=col, edgecolor='k', label=label) \n",
    "                   for label, col in color_map.items()]\n",
    "ax.legend(handles=legend_elements, title='sample_type')\n",
    "\n",
    "# labels and colorbar\n",
    "ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "ax.set_title('PCA')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# very interesting - are there some mis-labelled samples? other explanation?\n",
    "# note how much better separated the data are and note also PC1 now accounts for 29.1% of the variance in the data\n",
    "# note also how much tighter the normal tissue cluster is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94977670-a0ae-4d9c-a37a-ccfba2b29ad6",
   "metadata": {},
   "source": [
    "You may have noticed that the column header are not gene names so we're going to fix that by mapping (as we have done before in the course). You could use eUtils to do this or even bulk download the meta-data and use table merging, but we're going to use a nice package called \"mygene\" - (https://docs.mygene.info/projects/mygene-py/en/latest/)\n",
    "\n",
    "Converting Ensembl gene IDs (ENSG) to HGNC (HUGO Gene Nomenclature Committee) gene symbols is often a good practice as HGCN is an international standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2ff73-83bf-42b2-a305-e0537e669a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to do the gene mapping\n",
    "def rename_ensembl_to_gene_names(df, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Renames Ensembl gene IDs to gene names using mygene.\n",
    "\n",
    "    NB we chunk the requests to avoid hitting the rate limit.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with Ensembl gene IDs as columns.\n",
    "    chunk_size (int): Number of Ensembl IDs to query at a time.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with gene names as columns, excluding genes that couldn't be mapped.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Remove the `.number` suffix from ENSG IDs\n",
    "    df_copy.columns = df_copy.columns.str.split('.').str[0]\n",
    "\n",
    "    # Initialize mygene client\n",
    "    mg = mygene.MyGeneInfo()\n",
    "\n",
    "    # Split ENSG IDs into smaller chunks\n",
    "    def chunks(lst, n):\n",
    "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "    ensg_ids = df_copy.columns.tolist()\n",
    "    gene_mappings = []\n",
    "\n",
    "    unmapped_genes = []\n",
    "\n",
    "    # send requests in chunks\n",
    "    for chunk in chunks(ensg_ids, chunk_size):\n",
    "        result = mg.querymany(chunk, scopes='ensembl.gene', fields='symbol', species='human')\n",
    "        gene_mappings.extend(result)\n",
    "\n",
    "    # Create a mapping from ENSG to gene symbol, handle missing mappings\n",
    "    ensg_to_gene = {item['query']: item.get('symbol', None) for item in gene_mappings}\n",
    "    \n",
    "    # Log the unmapped genes\n",
    "    batch_unmapped_genes = [gene for gene in ensg_ids if ensg_to_gene.get(gene) is None]\n",
    "    if batch_unmapped_genes:\n",
    "        # Add unmapped genes to the list\n",
    "        unmapped_genes.extend(batch_unmapped_genes)\n",
    "\n",
    "    # Filter the DataFrame to only include columns that have been mapped\n",
    "    df_filtered = df_copy.loc[:, df_copy.columns.isin(ensg_to_gene.keys())]\n",
    "\n",
    "    # Further filter to ensure we have the same number of columns as mapped gene names\n",
    "    df_filtered = df_filtered.loc[:, [ensg for ensg in df_filtered.columns if ensg_to_gene[ensg] is not None]]\n",
    "\n",
    "    # Assign new column names\n",
    "    df_filtered.columns = [ensg_to_gene[ensg] for ensg in df_filtered.columns]\n",
    "\n",
    "    # Handle duplicate gene names by aggregating them (e.g., by taking the mean)\n",
    "    df_final = df_filtered.T.groupby(df_filtered.columns).mean().T\n",
    "\n",
    "    return df_final, set(unmapped_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05532782-2328-42e7-a7a7-91380f248c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the ensembl gene IDs to gene names\n",
    "final_genes,unmapped_genes = rename_ensembl_to_gene_names(significant_genes)\n",
    "print(f'{len(unmapped_genes)} were not mapped to gene names.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c958625-af02-4f5b-a728-cf849d32158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the first few rows of the renamed DataFrame\n",
    "final_genes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2aab89",
   "metadata": {},
   "source": [
    "We now want to create correlation matrices based on gene profiles to see how groups of genes vary between cancerous and non-cancerous samples\n",
    "\n",
    "There are a few correlation metrics one could consider:\n",
    "- [Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)  \n",
    "  - O(n^2) complexity, fast for large datasets\n",
    "- [Spearman](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)\n",
    "  -  O(n^2 log n) complexity, relatively fast but can be slower than Pearson\n",
    "- [Absolute biweight midcorrelation](https://en.wikipedia.org/wiki/Biweight_midcorrelation)\n",
    "  - Robust but slower than Pearson and Spearman, suitable for datasets with outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae978e0",
   "metadata": {},
   "source": [
    "We now want to establish whether there are any strong relationships between the expression levels of the genes in the different samples. We can do this by calculating the correlation between their expression values across samples. We can use this correlation matrix as an adjacency matrix to build a network.\n",
    "\n",
    "- nodes: genes  \n",
    "- edges: highly correlated genes (above a given threshold)\n",
    "- edge-weights: correlation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee5d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're using some functions that we have defined in the accompanying functions file\n",
    "\n",
    "# Dictionary to store different correlation matrices\n",
    "correlation_matrices = {}\n",
    "\n",
    "# Pearson correlation - O(n^2) complexity, fast for large datasets\n",
    "correlation_matrices['pearson'] = final_genes.corr(method='pearson')\n",
    "\n",
    "# Spearman rank correlation -  O(n^2 log n) complexity, relatively fast but can be slower than Pearson\n",
    "correlation_matrices['spearman'] = final_genes.corr(method='spearman')\n",
    "\n",
    "# Biweight midcorrelation -  Robust but slower than Pearson and Spearman, suitable for datasets with outliers\n",
    "correlation_matrices['biweight_midcorrelation'] = gf.calc_abs_bicorr(final_genes)\n",
    "\n",
    "# Print the keys of the correlation matrices to verify\n",
    "print(\"Correlation matrices calculated:\")\n",
    "print(correlation_matrices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot these in a multi-panel plot\n",
    "gf.plot_correlation_matrices(correlation_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8be3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create an actual graph from the pearson correlation matrix\n",
    "G = gf.create_graph_from_correlation(correlation_matrices['pearson'], threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32bd5b",
   "metadata": {},
   "source": [
    "We are going to define a function `create_graph_from_correlation` to make networks from correlation matrices.\n",
    "\n",
    "The function starts by creating an empty graph G. Then iterates through the columns of the correlation matrix and adds each column name as a node in the graph. This means each gene (or feature) in your dataset becomes a node in the graph.\n",
    "\n",
    "The function iterates over the upper triangle of the correlation matrix (excluding the diagonal) to avoid redundancy and self-loops. Remembering that this is an undirected graph so is symmetric.\n",
    "\n",
    "For each pair of nodes (i, j), it checks if the absolute value of the correlation coefficient between them is greater than or equal to the specified threshold.\n",
    "\n",
    "If the condition is met, an edge is added between the nodes i and j with the correlation coefficient as the weight of the edge. This signifies a strong correlation (positive or negative) between the two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e40213-7ce1-48c8-9323-52a18a50c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the correlation matrix using a specified threshold\n",
    "def create_graph_from_correlation(correlation_matrix, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Creates a graph from a correlation matrix using a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    correlation_matrix (pd.DataFrame): DataFrame containing the correlation matrix.\n",
    "    threshold (float): Threshold for including edges based on correlation value.\n",
    "\n",
    "    Returns:\n",
    "    G (nx.Graph): Graph created from the correlation matrix.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes\n",
    "    for node in correlation_matrix.columns:\n",
    "        G.add_node(node)\n",
    "\n",
    "    # Add edges with weights above the threshold\n",
    "    for i in range(correlation_matrix.shape[0]):\n",
    "        for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "            if i != j:  # Ignore the diagonal elements\n",
    "                weight = correlation_matrix.iloc[i, j]\n",
    "                if abs(weight) >= threshold:\n",
    "                    G.add_edge(correlation_matrix.index[i], correlation_matrix.columns[j], weight=weight)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d8036-f692-48d5-b933-faffbd3c6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the Pearson correlation matrix with a threshold of 0.75\n",
    "pearson_graph = create_graph_from_correlation(correlation_matrices['pearson'], threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3ac37",
   "metadata": {},
   "source": [
    "Now let's go through a few useful NetworkX functions and create a `print_graph_info()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb68c6-49f6-4f21-a91e-aa15ee100576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print basic information about the graph\n",
    "def print_graph_info(G):\n",
    "    \"\"\"\n",
    "    Print basic information about a NetworkX graph.\n",
    "\n",
    "    \n",
    "    Parameters:\n",
    "    G (nx.Graph): The NetworkX graph.\n",
    "    \"\"\"\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "    print(\"Sample nodes:\", list(G.nodes)[:10])  # Print first 10 nodes as a sample\n",
    "    print(\"Sample edges:\", list(G.edges(data=True))[:10])  # Print first 10 edges as a sample\n",
    "    \n",
    "    info_str = \"Graph type: \"\n",
    "    is_directed = G.is_directed()\n",
    "    if is_directed:\n",
    "        info_str += \"directed\"\n",
    "    else:\n",
    "        info_str += \"undirected\"\n",
    "    print(info_str)\n",
    "\n",
    "    # Check for self-loops\n",
    "    self_loops = list(nx.selfloop_edges(G))\n",
    "    if self_loops:\n",
    "        print(f\"Number of self-loops: {len(self_loops)}\")\n",
    "        print(\"Self-loops:\", self_loops)\n",
    "    else:\n",
    "        print(\"No self-loops in the graph.\")\n",
    "\n",
    "    # density of the graph\n",
    "    density = nx.density(G)\n",
    "    print(f\"Graph density: {density}\")\n",
    "\n",
    "    # Find and print the number of connected components\n",
    "    num_connected_components = nx.number_connected_components(G)\n",
    "    print(f\"Number of connected components: {num_connected_components}\")\n",
    "\n",
    "    # Calculate and print the clustering coefficient of the graph\n",
    "    clustering_coeff = nx.average_clustering(G)\n",
    "    print(f\"Average clustering coefficient: {clustering_coeff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1131c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graph_info(pearson_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc77a6-a40f-453b-94a8-a06f81c43695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the graph\n",
    "gf.visualise_graph(pearson_graph, title='Pearson Correlation Network (Threshold = 0.7)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09489b77-7091-4ac6-aead-4d76f578556b",
   "metadata": {},
   "source": [
    "We now have the base gene correlation network but we can see that there are a lot of orphans (due to the threshold filterinf and so need to clean the network up. We can use functions from NetworkX for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d80a8e-4d7e-4d67-aa43-251d11c8e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the graph\n",
    "def clean_graph(G, degree_threshold=1, keep_largest_component=True):\n",
    "    \"\"\"\n",
    "    Cleans the graph by performing several cleaning steps:\n",
    "    - Removes unconnected nodes (isolates)\n",
    "    - Removes self-loops\n",
    "    - Removes nodes with a degree below a specified threshold\n",
    "    - Keeps only the largest connected component (optional)\n",
    "\n",
    "    Parameters:\n",
    "    G (nx.Graph): The NetworkX graph to clean.\n",
    "    degree_threshold (int): Minimum degree for nodes to keep.\n",
    "    keep_largest_component (bool): Whether to keep only the largest connected component.\n",
    "\n",
    "    Returns:\n",
    "    G (nx.Graph): Cleaned graph.\n",
    "    \"\"\"\n",
    "    G = G.copy()  # Work on a copy of the graph to avoid modifying the original graph\n",
    "\n",
    "    # Remove self-loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    # Remove nodes with no edges (isolates)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "    # Remove nodes with degree below the threshold\n",
    "    low_degree_nodes = [node for node, degree in dict(G.degree()).items() if degree < degree_threshold]\n",
    "    G.remove_nodes_from(low_degree_nodes)\n",
    "\n",
    "    # Keep only the largest connected component\n",
    "    if keep_largest_component:\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ead5d-38fc-43c7-bd82-6a9dee7db91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the graph by removing unconnected nodes\n",
    "pearson_graph_cleaned = clean_graph(pearson_graph,\n",
    "                                    degree_threshold=1,\n",
    "                                    keep_largest_component=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f2c40-de71-4323-b526-2c14575270c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the cleaned graph\n",
    "# NB this is now tractable quickly as the graph is much smaller\n",
    "gf.visualise_graph(pearson_graph_cleaned, title='Pearson Correlation Network - Cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1cb0f-7b9c-4550-937b-7354d8d3285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can re-use the function to print the graph information\n",
    "print_graph_info(pearson_graph_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d1f7a-e96f-4d17-b102-3b522b5471e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the graph by keeping only the largest connected component\n",
    "pearson_graph_pruned = clean_graph(pearson_graph,\n",
    "                                    degree_threshold=1,\n",
    "                                    keep_largest_component=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8da99-c27b-43c5-b5bf-53702c3e8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf.visualise_graph(pearson_graph_pruned, title='Pearson Correlation Network - Pruned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca45d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can re-use the function to print the graph information\n",
    "print_graph_info(pearson_graph_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b854688-aa87-4b76-8e40-81b5ebe5ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of edge weights\n",
    "gf.visualise_edge_weight_distribution(pearson_graph_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00815c91-639a-4fe2-8333-291c2614e7fe",
   "metadata": {},
   "source": [
    "With sparsification we aim to reduce the number of edges in a network while preserving important structural properties.\n",
    "\n",
    "- Edge Sampling: Randomly removes a fraction of edges.\n",
    "- Thresholding: Removes edges with weights below a certain threshold.\n",
    "- Degree-based Sparsification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d1759-bd06-4723-95f3-2afce45d2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply remove the edges below a certain edge-weight threshold\n",
    "def threshold_sparsification(graph, threshold):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by removing edges below the specified weight threshold.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    threshold (float): The weight threshold.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    sparsified_graph = nx.Graph()\n",
    "    sparsified_graph.add_nodes_from(graph_copy.nodes(data=True))\n",
    "    sparsified_graph.add_edges_from((u, v, d) for u, v, d in graph_copy.edges(data=True) if d.get('weight', 0) >= threshold)\n",
    "    return sparsified_graph\n",
    "\n",
    "# keep the specified top quantile of edges by edge-weight\n",
    "def top_percentage_sparsification(graph, top_percentage):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by keeping the top percentage of edges by weight.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    top_percentage (float): The percentage of top-weight edges to keep.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    sorted_edges = sorted(graph_copy.edges(data=True), key=lambda x: x[2].get('weight', 0), reverse=True)\n",
    "    top_edges_count = max(1, int(len(sorted_edges) * (top_percentage / 100)))\n",
    "    sparsified_graph = nx.Graph()\n",
    "    sparsified_graph.add_nodes_from(graph_copy.nodes(data=True))\n",
    "    sparsified_graph.add_edges_from(sorted_edges[:top_edges_count])\n",
    "    return sparsified_graph\n",
    "\n",
    "\n",
    "# remove nodes with degree below a certain threshold\n",
    "def remove_by_degree(graph, min_degree):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by removing nodes with degree below the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    min_degree (int): The minimum degree threshold.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    nodes_to_remove = [node for node, degree in dict(graph_copy.degree()).items() if degree < min_degree]\n",
    "    \n",
    "    graph_copy.remove_nodes_from(nodes_to_remove)\n",
    "    return graph_copy\n",
    "\n",
    "# use KNN sparsification to keep up to only the top N edges for a node\n",
    "def knn_sparsification(graph, k):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by keeping only the top-k edges with the highest weights for each node.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    k (int): The number of nearest neighbors to keep for each node.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    sparsified_graph = nx.Graph()\n",
    "    sparsified_graph.add_nodes_from(graph_copy.nodes(data=True))\n",
    "    \n",
    "    for node in graph_copy.nodes():\n",
    "        edges = sorted(graph_copy.edges(node, data=True), key=lambda x: x[2].get('weight', 0), reverse=True)\n",
    "        sparsified_graph.add_edges_from(edges[:k])\n",
    "    \n",
    "    return sparsified_graph\n",
    "\n",
    "# create a minimum spanning tree\n",
    "def spanning_tree_sparsification(graph):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by creating a minimum spanning tree.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    return nx.minimum_spanning_tree(graph_copy, weight='weight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3c09b-363a-49cc-99bb-2406b89b59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a dictionary to store graphs\n",
    "graphs = {}\n",
    "# Store the original graph for comparison\n",
    "graphs['original'] = pearson_graph_pruned.copy()\n",
    "\n",
    "# Apply sparsification methods to the original graph\n",
    "graphs['knn_5'] = knn_sparsification(graphs['original'], k=5)\n",
    "# graphs['threshold'] = threshold_sparsification(graphs['original'], threshold=0.82)\n",
    "# graphs['top_10_percent'] = top_percentage_sparsification(graphs['original'], top_percentage=10)\n",
    "# graphs['degree_below_3'] = remove_by_degree(graphs['original'], min_degree=3)\n",
    "# graphs['spanning_tree'] = spanning_tree_sparsification(graphs['original'])\n",
    "\n",
    "\n",
    "# Visualise the graphs after sparsification\n",
    "gf.visualise_graph(graphs['original'], 'Original Graph')\n",
    "gf.visualise_graph(graphs['knn_5'], 'K-Nearest Neighbors (k=5)')\n",
    "# gf.visualise_graph(graphs['threshold'], 'Thresholded Graph (weight > 0.82)')\n",
    "# gf.visualise_graph(graphs['top_10_percent'], 'Top 10% Edges by Weight')\n",
    "# gf.visualise_graph(graphs['degree_below_3'], 'Degree Below 3')\n",
    "# gf.visualise_graph(graphs['spanning_tree'], 'Minimum Spanning Tree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef861253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the information of the KNN sparsified graph\n",
    "print_graph_info(graphs['knn_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b493f5-1b7e-4039-aa85-8ff27b1440c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to analyse the effect of different k values on the network properties\n",
    "def analyse_knn_effect(graph, k_values):\n",
    "    \"\"\"\n",
    "    Analyses the effect of different k values on the network properties.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    k_values (list): List of k values to use for sparsification.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the analysis results.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'k': [],\n",
    "        'num_edges': [],\n",
    "        'avg_degree': [],\n",
    "        'avg_clustering': [],\n",
    "        'num_connected_components': [],\n",
    "    }\n",
    "    \n",
    "    for k in k_values:\n",
    "        sparsified_graph = knn_sparsification(graph, k)\n",
    "        num_edges = sparsified_graph.number_of_edges()\n",
    "        avg_degree = sum(dict(sparsified_graph.degree()).values()) / sparsified_graph.number_of_nodes()\n",
    "        avg_clustering = nx.average_clustering(sparsified_graph)\n",
    "        num_connected_components = nx.number_connected_components(sparsified_graph)\n",
    "        \n",
    "        results['k'].append(k)\n",
    "        results['num_edges'].append(num_edges)\n",
    "        results['avg_degree'].append(avg_degree)\n",
    "        results['avg_clustering'].append(avg_clustering)\n",
    "        results['num_connected_components'].append(num_connected_components)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# plot the analysis of the effect of different k values on network properties\n",
    "def plot_knn_analysis(df):\n",
    "    \"\"\"\n",
    "    Plots the analysis of the effect of different k values on network properties.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the analysis results.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    axes[0, 0].plot(df['k'], df['num_edges'], marker='o')\n",
    "    axes[0, 0].set_title('Number of Edges vs k')\n",
    "    axes[0, 0].set_xlabel('k')\n",
    "    axes[0, 0].set_ylabel('Number of Edges')\n",
    "    \n",
    "    axes[0, 1].plot(df['k'], df['avg_degree'], marker='o')\n",
    "    axes[0, 1].set_title('Average Degree vs k')\n",
    "    axes[0, 1].set_xlabel('k')\n",
    "    axes[0, 1].set_ylabel('Average Degree')\n",
    "    \n",
    "    axes[1, 0].plot(df['k'], df['avg_clustering'], marker='o')\n",
    "    axes[1, 0].set_title('Average Clustering Coefficient vs k')\n",
    "    axes[1, 0].set_xlabel('k')\n",
    "    axes[1, 0].set_ylabel('Average Clustering Coefficient')\n",
    "    \n",
    "    axes[1, 1].plot(df['k'], df['num_connected_components'], marker='o')\n",
    "    axes[1, 1].set_title('Number of Connected Components vs k')\n",
    "    axes[1, 1].set_xlabel('k')\n",
    "    axes[1, 1].set_ylabel('Number of Connected Components')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = list(range(1, 11))  # Different k values to analyze\n",
    "analysis_results = analyse_knn_effect(graphs['original'], k_values)\n",
    "\n",
    "# Plot the analysis results\n",
    "plot_knn_analysis(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b33e51-c495-46f1-b7ca-db68b0374ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to look at the top nodes based on degree\n",
    "def get_highest_degree_nodes(graph, top_n=10):\n",
    "    \"\"\"\n",
    "    Returns the nodes with the highest degree in the graph.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The NetworkX graph.\n",
    "    top_n (int): The number of top nodes to return.\n",
    "\n",
    "    Returns:\n",
    "    List of tuples: Each tuple contains a node and its degree.\n",
    "    \"\"\"\n",
    "    degrees = dict(graph.degree())\n",
    "    sorted_degrees = sorted(degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_degrees[:top_n]\n",
    "\n",
    "# gather some information about nodes using mygene\n",
    "def fetch_gene_info(gene_list):\n",
    "    \"\"\"\n",
    "    Fetches gene information from MyGene.info.\n",
    "\n",
    "    Parameters:\n",
    "    gene_list (list): List of gene symbols or Ensembl IDs.\n",
    "\n",
    "    Returns:\n",
    "    list: List of dictionaries containing gene information.\n",
    "    \"\"\"\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    gene_info = mg.querymany(gene_list, scopes='symbol,ensembl.gene', \n",
    "                             fields='name,symbol,entrezgene,summary,disease,pathway', \n",
    "                             species='human')\n",
    "    return gene_info\n",
    "\n",
    "# combined function to report node information alongside gene metadata\n",
    "def print_gene_info_with_degree(top_genes_with_degrees, gene_info):\n",
    "    \"\"\"\n",
    "    Prints gene information including the degree.\n",
    "\n",
    "    Parameters:\n",
    "    top_genes_with_degrees (list): List of tuples containing gene symbols and their degrees.\n",
    "    gene_info (list): List of dictionaries containing gene information.\n",
    "    \"\"\"\n",
    "    for gene, degree in top_genes_with_degrees:\n",
    "        info = next((item for item in gene_info if item['query'] == gene), None)\n",
    "        if info:\n",
    "            print(f\"Gene Symbol: {info.get('symbol', 'N/A')}\")\n",
    "            print(f\"Degree: {degree}\")\n",
    "            print(f\"Gene Name: {info.get('name', 'N/A')}\")\n",
    "            print(f\"Entrez ID: {info.get('entrezgene', 'N/A')}\")\n",
    "            print(f\"Summary: {info.get('summary', 'N/A')}\")\n",
    "            if 'disease' in info:\n",
    "                diseases = ', '.join([d['term'] for d in info['disease']])\n",
    "                print(f\"Diseases: {diseases}\")\n",
    "            else:\n",
    "                print(\"Diseases: N/A\")\n",
    "            if 'pathway' in info:\n",
    "                pathways = []\n",
    "                if isinstance(info['pathway'], dict):\n",
    "                    for key in info['pathway']:\n",
    "                        pathway_data = info['pathway'][key]\n",
    "                        if isinstance(pathway_data, list):\n",
    "                            pathways.extend([p['name'] for p in pathway_data if 'name' in p])\n",
    "                        elif isinstance(pathway_data, dict) and 'name' in pathway_data:\n",
    "                            pathways.append(pathway_data['name'])\n",
    "                        elif isinstance(pathway_data, str):\n",
    "                            pathways.append(pathway_data)\n",
    "                print(f\"Pathways: {', '.join(pathways) if pathways else 'N/A'}\")\n",
    "            else:\n",
    "                print(\"Pathways: N/A\")\n",
    "            print(\"-\" * 40)\n",
    "        else:\n",
    "            print(f\"Gene not found: {gene}\")\n",
    "            print(f\"Degree: {degree}\")\n",
    "            print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f951b-f7ec-4d0c-9034-e6d9690f4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10 genes with the highest degree in the pruned graph using get_highest_degree_nodes\n",
    "top_genes_with_degrees = get_highest_degree_nodes(pearson_graph_pruned, top_n=10)\n",
    "gene_symbols = [gene for gene, degree in top_genes_with_degrees]\n",
    "\n",
    "# get gene information with fetch_gene_info\n",
    "gene_info = fetch_gene_info(gene_symbols)\n",
    "\n",
    "# print gene information including degree\n",
    "print_gene_info_with_degree(top_genes_with_degrees, gene_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
